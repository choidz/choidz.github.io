# LM Studio로 로컬 PC에서 LLM 모델 실행하기

---

**LM Studio란 무엇인가**

LM Studio는 사용자가 인터넷 연결 없이 자신의 컴퓨터에서 직접 대형 언어 모델(LLM)을 실행할 수 있도록 해주는 프로그램입니다. Hugging Face에 공개된 오픈소스 AI 모델을 쉽게 다운로드하고, 구성하며, 활용할 수 있습니다. 개인정보 보호와 데이터 보안이 중요한 사용자들에게 특히 유용한 솔루션입니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-00-4bbcb4ae.png)

**LM Studio 설치 및 시스템 요구사항**

LM Studio는 모든 주요 운영체제에서 구동 가능합니다. 공식 웹사이트(https://lmstudio.ai/)에 접속하면 Windows, macOS, Linux용 설치 파일을 다운로드할 수 있습니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-01-4f0fb87b.jpeg)

설치 전에 최소 사양을 확인해야 합니다. macOS의 경우 M1/M2/M3 칩을 탑재한 Apple Silicon Mac이 필요하며 macOS 13.6 이상이어야 합니다. Windows와 Linux PC는 AVX2를 지원하는 프로세서가 필요합니다. 모든 플랫폼에서 최소 16GB 이상의 RAM을 권장하며, PC의 경우 6GB 이상의 VRAM을 권장합니다. NVIDIA와 AMD GPU를 지원합니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-02-14f31a6d.jpeg)

다운로드한 파일을 더블클릭하여 실행하면 설치 과정이 시작됩니다. 설치 완료 후 프로그램을 실행하면 현재 버전 정보를 표시하는 창이 나타나며, Discord 커뮤니티를 통해 사용자들로부터 도움을 받을 수 있습니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-03-83c3383d.png)

**LM Studio의 주요 기능**

**1\. 모델 다운로드 및 검색 (Home 화면)**

LM Studio의 첫 화면에는 Google 검색 바처럼 긴 검색창이 있습니다. 이곳에서 원하는 오픈소스 LLM을 검색할 수 있습니다. 인기 있는 모델들이 추천으로 표시되며, 사용자는 Llama, Qwen 등 다양한 모델을 검색하고 다운로드할 수 있습니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-04-43c78557.jpeg)

​

검색 결과는 좌측과 우측으로 깔끔하게 표시됩니다. 좌측에는 LLM 버전별로 업로드된 모델들이 나열되며, 각 모델의 이름, 업로드 날짜, 다운로드 수, 좋아요 개수를 확인할 수 있습니다. 날짜별, 다운로드 수 등으로 정렬 기능도 제공됩니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-05-90dfeb98.jpeg)

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-06-090f44ea.jpeg)

**양자화(Quantization) 레벨 이해하기**

우측에 표시되는 다양한 모델 옵션들은 2~8비트 양자화 레벨별로 구분됩니다. 양자화는 모델의 크기를 줄이기 위해 가중치를 더 낮은 비트로 압축하는 기술입니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-07-08e81fd4.jpeg)

**2~4비트 양자화(Q2_K, Q3_K, Q4_K)** 는 고도로 압축되어 더 작은 크기와 낮은 메모리 사용량을 특징으로 합니다. 제한된 RAM을 가진 시스템에 적합하지만 일부 정확도를 희생해야 합니다. Q4_0에서는 숫자 블록이 하나의 스케일링 인수로 표현되며, Q4_1에서는 스케일링 인수와 오프셋 인수를 모두 사용하여 정확도를 향상시킵니다.

**5~6비트 양자화(Q5_K, Q6_K)** 는 크기와 정확도 사이에서 더 정교한 균형을 이루어, 중간 메모리 용량을 가진 시스템에 유리한 선택입니다. **8비트 양자화(Q8_0)** 는 원본 모델의 정확도에 더 가깝지만 더 많은 메모리가 요구되며, 충분한 RAM을 가진 시스템에 적합합니다.

일반적으로 자신의 컴퓨터 사양에 맞게 선택하면 되는데, 다행히 LM Studio는 사용자의 사양을 자동으로 인식하고 다운로드 버튼 옆에 충분한 사양일 경우 초록색으로 'full cpu offload possible' 글자를 표시합니다.

![](/images/posts/lm-studio로-로컬-pc에서-llm-모델-실행하기-08-6ad86606.png)

**2\. 모델 관리 (My Models 화면)**

다운로드한 모델은 왼쪽 사이드바의 폴더 모양 아이콘 'My Models'에서 관리할 수 있습니다. 모델 용량이 크기 때문에 여러 개 다운로드하면 저장 공간이 빠르게 차므로, 사용하지 않거나 마음에 들지 않는 모델을 삭제할 수 있습니다.

**3\. AI Chat 기능 (대화형 인터페이스)**

말풍선 아이콘의 AI Chat 탭에서는 다운로드한 모델을 직접 사용할 수 있습니다. 콘솔 게임처럼 사용하려는 LLM을 로드(load)하고, 사용을 끝낸 후 다른 모델을 사용하려면 기존 모델을 언로드(eject)해야 합니다.

'Select a model' 버튼을 클릭하여 다운로드 완료한 모델들을 확인하고 선택할 수 있습니다. 모델 로드 후 상태창에서 다양한 성능 지표를 확인할 수 있습니다:

  * **time to first token** : 첫 번째 토큰 생성까지 걸린 시간

  * **gen time** : 전체 텍스트 생성에 걸린 총시간

  * **speed** : 초당 생성된 토큰의 수

  * **stop reason** : 텍스트 생성이 멈춘 이유

  * **gpu layers** : GPU에서 처리된 모델 레이어의 수

  * **cpu threads** : 사용된 CPU 스레드의 수

  * **token count** : 생성된 토큰 수와 최대 허용 토큰 수의 비율

오른쪽 상단의 톱니바퀴 설정 아이콘을 클릭하면 System Prompt와 Temperature 등 모델을 미세조정할 수 있습니다.

**4\. Playground 기능 (멀티 모델 세션)**

Playground는 여러 모델을 한 번에 로드하고, 동일한 프롬프트로 다른 모델들을 평가할 수 있는 기능입니다. API를 사용하여 다중 LLM 앱을 구축할 수도 있습니다. 다만 컴퓨터 성능이 우수하지 않으면 시스템이 느려질 수 있으므로 주의가 필요합니다.

**로컬 서버를 이용한 챗봇 구축**

**Local Server 기능**

네 번째 탭의 Local Server에서는 다운로드한 LLM 모델을 로컬 서버에 연결하여 사용할 수 있습니다. 이 기능은 OpenAI API 엔드포인트를 모방하는 로컬 HTTP 서버를 시작할 수 있게 해줍니다. 쉽게 말해, OpenAI 서비스처럼 작동하는 AI 서비스를 자신의 로컬 컴퓨터에서 실행할 수 있다는 의미입니다.

최상단에서 모델을 로드하면 파란색으로 모델이 로컬 서버와 연결된 것을 확인할 수 있습니다. Server logs를 통해 서버의 작동 상태를 모니터링할 수 있습니다.

**Local Server의 3가지 주요 기능**

**1\. Local Inference Server** 는 컴퓨터를 작은 AI 서버로 만들어주는 도구입니다. 모든 작업 과정이 로컬 컴퓨터 안에서 이루어지므로 인터넷 연결 없이도 AI를 사용할 수 있습니다. 'Start Server'와 'Stop Server' 버튼으로 서버를 연결하고 끊을 수 있습니다.

**2\. Embedding Model Settings** 는 텍스트를 숫자 벡터로 변환하는 특별한 AI 모델을 설정하는 부분입니다. 임베딩 모델은 주로 RAG(Retrieval-Augmented Generation) 시스템에서 중요한 역할을 합니다. RAG는 AI가 질문에 답할 때 외부 정보를 검색하여 더 정확하고 풍부한 답변을 생성하는 기술입니다. 'nomic-ai/nomic embed text v1.5' 모델을 다운로드받아 사용할 수 있으며, 일반적인 대화에서는 필요 없지만 대규모 문서 검색이나 정보 추출이 필요할 때 유용합니다.

**3\. Examples** 는 LM Studio에서 제공하는 다양한 LLM 모델 접근 방식입니다. hello world(curl), chat(python), ai assistant(python), vision(python), embeddings(python) 등 다양한 예제 코드를 제공합니다.

**Python을 이용한 로컬 챗봇 구현**

LM Studio의 Local Server 기능을 활용하여 Python으로 로컬 챗봇을 만들 수 있습니다. 먼저 'Start Server'를 눌러 서버에 연결한 후, 'Chat(Python)' 예제에서 코드를 복사합니다.

이 코드는 OpenAI 라이브러리를 사용하여 로컬 서버에 연결하고, 사용자 정의 프롬프트를 통해 특정 역할을 수행하는 챗봇을 구현합니다. 서버에 연결하지 않고 실행하면 연결 오류가 발생합니다.

**성능 및 실제 사용 사례**

Llama 3.1-7B 모델을 사용하여 "Roast Beef Smoked Gouda Grilled Cheese" 레시피를 요청했을 때, Q5_K_M 양자화 모델은 약 1분 30초에 상세한 답변을 생성했습니다. 해산물 크림 파스타 레시피 요청도 약 1분 33초 소요되었습니다.

응답 속도는 ChatGPT나 Claude AI 같은 유료 서비스보다 느리지만, 개인정보 유출이나 데이터 보안에 민감한 사용자들에게는 충분한 가치가 있습니다. RAG 기법을 추가로 적용하면 성능을 더욱 향상시킬 수 있습니다.

**결론**

LM Studio는 로컬 PC에서 강력한 LLM을 자유롭게 실행할 수 있는 훌륭한 도구입니다. 설치부터 모델 다운로드, 대화형 인터페이스 사용, 로컬 서버 구축까지 직관적인 UI로 모든 과정을 지원합니다. 특히 Python을 활용한 커스텀 챗봇 구축이 가능하여, 기업이나 개인의 특정 요구사항에 맞는 AI 솔루션을 만들 수 있습니다. 프라이버시와 보안을 중시하는 사용자라면 LM Studio는 매우 실용적인 선택입니다.

​

​

#LMStudio #로컬LLM #Llama #파이썬챗봇 #AI개발

[원문 보기](https://blog.naver.com/choidz_/224084844660?fromRss=true&trackingCode=rss)
