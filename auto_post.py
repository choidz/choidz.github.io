import json
import os
import re
import subprocess
from datetime import datetime
from pathlib import Path
import requests
import feedparser
import html2text
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

NAVER_RSS_URL = os.getenv("NAVER_RSS_URL")
POSTS_DIR = Path("public/posts")
MANIFEST_PATH = POSTS_DIR / "index.json"
GIT_USER_NAME = os.getenv("GIT_USER_NAME")
GIT_USER_EMAIL = os.getenv("GIT_USER_EMAIL")

if not NAVER_RSS_URL:
    print("âŒ NAVER_RSS_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# âœ… html2text ê¸°ë³¸ ì„¤ì •
converter = html2text.HTML2Text()
converter.ignore_links = False
converter.ignore_images = False
converter.body_width = 0
converter.single_line_break = False
converter.protect_links = True
converter.mark_code = False
converter.unicode_snob = True
converter.skip_internal_links = False
converter.escape_snob = False  # ì¤‘ìš”: --- ë°©ì§€

slug_pattern = re.compile(r"[^a-z0-9]+")
whitespace_re = re.compile(r"\s+")

POSTS_DIR.mkdir(parents=True, exist_ok=True)

existing_manifest = []
if MANIFEST_PATH.exists():
    try:
        existing_manifest = json.loads(MANIFEST_PATH.read_text(encoding="utf-8"))
    except json.JSONDecodeError:
        existing_manifest = []

posts_by_slug = {item["slug"]: item for item in existing_manifest if "slug" in item}
updated = False


def fetch_full_naver_post(link: str):
    # ë¸”ë¡œê·¸ ID, ê¸€ë²ˆí˜¸ ì¶”ì¶œ
    m = re.search(r"blog.naver.com/([^/]+)/(\d+)", link)
    if not m:
        print(f"âŒ ë§í¬ íŒŒì‹± ì‹¤íŒ¨: {link}")
        return "", []
    blog_id, log_no = m.groups()

    mobile_url = f"https://m.blog.naver.com/{blog_id}/{log_no}"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(mobile_url, headers=headers, timeout=10)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ
    container = soup.select_one("div.se-main-container") or soup.select_one("div.se_component_wrap")

    # âœ… __NEXT_DATA__ ë‚´ë¶€ JSONì—ì„œ íƒœê·¸ ì¶”ì¶œ
    script_tag = soup.find("script", id="__NEXT_DATA__")
    tags = []
    if script_tag and script_tag.string:
        try:
            data = json.loads(script_tag.string)
            # êµ¬ì¡°: props.pageProps.postInfo.tags (2025ë…„ í˜„ì¬)
            tags_data = (
                data.get("props", {})
                    .get("pageProps", {})
                    .get("postInfo", {})
                    .get("tags", [])
            )
            for tag_item in tags_data:
                tag_text = tag_item.get("tagName") or tag_item.get("name") or ""
                if tag_text:
                    if not tag_text.startswith("#"):
                        tag_text = f"#{tag_text}"
                    tags.append(tag_text)
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

    print(f"âœ… {mobile_url} â†’ íƒœê·¸: {tags}")
    return (str(container) if container else ""), tags


# âœ… í‘œ ì…€ ì´ìŠ¤ì¼€ì´í”„
def escape_md_table_cell(text: str) -> str:
    if not text:
        return ""
    text = text.replace("\\", "\\\\")
    text = text.replace("|", "\\|")
    text = re.sub(r"\s+", " ", text).strip()
    return text


# âœ… í‘œ ì •ë ¬ í•¨ìˆ˜
def format_markdown_table(rows: list[list[str]]) -> str:
    col_count = max(len(r) for r in rows)
    col_widths = [0] * col_count
    for r in rows:
        for i, c in enumerate(r):
            col_widths[i] = max(col_widths[i], len(c))

    def fmt_row(cells):
        return "| " + " | ".join(c.ljust(col_widths[i]) for i, c in enumerate(cells)) + " |"

    header = fmt_row(rows[0])
    divider = "| " + " | ".join("-" * col_widths[i] for i in range(col_count)) + " |"
    body = [fmt_row(r) for r in rows[1:]]
    return "\n".join([header, divider] + body)

# âœ… ë„¤ì´ë²„ ì´ë¯¸ì§€ URL ì •ê·œí™” (ì¸ë„¤ì¼ â†’ ì›ë³¸ í¬ê¸°)
def normalize_naver_image_url(url: str) -> str:
    """ë„¤ì´ë²„ ì´ë¯¸ì§€ URLì—ì„œ type íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ì—¬ ì›ë³¸ í¬ê¸°ë¡œ ë³€ê²½"""
    if not url:
        return url
    # type=w80_blur, type=w420 ë“±ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°
    # ë˜ëŠ” ë” í° í¬ê¸°ë¡œ ë³€ê²½ (type=w966 ì •ë„ê°€ ì ë‹¹)
    url = re.sub(r'\?type=w\d+(_blur)?', '', url)
    url = re.sub(r'&type=w\d+(_blur)?', '', url)
    return url


# âœ… HTML ì •ì œ ë° ìš”ì•½ ì¶”ì¶œ
def sanitize_html(html: str) -> tuple[str, str, list[str], list[str]]:
    soup = BeautifulSoup(html or "", "html.parser")
    tables = []
    code_blocks = []

    # âœ… ì´ë¯¸ì§€ URL ì •ê·œí™” (ì¸ë„¤ì¼ â†’ ì›ë³¸)
    for img in soup.find_all("img"):
        src = img.get("src") or img.get("data-src") or ""
        if "pstatic.net" in src or "blogfiles.naver" in src:
            img["src"] = normalize_naver_image_url(src)
            # data-srcë„ ì •ë¦¬
            if img.get("data-src"):
                img["data-src"] = normalize_naver_image_url(img["data-src"])

    def has_class_fragment(tag, keyword):
        classes = tag.get("class") or []
        return any(keyword in cls for cls in classes)

    # âœ… ë„¤ì´ë²„ se-table êµ¬ì¡°ë¥¼ ì‹¤ì œ table íƒœê·¸ë¡œ ë³€í™˜
    for se_table in soup.find_all(
        lambda tag: tag.name == "div" and has_class_fragment(tag, "se-table")
    ):
        rows = []
        row_divs = se_table.select("div.se-table-row")
        for row_div in row_divs:
            cols = []
            for col_div in row_div.select("div.se-table-col"):
                text = col_div.get_text(" ", strip=True)
                text = text.replace("`", "")
                text = escape_md_table_cell(text)
                cols.append(text)
            if cols:
                rows.append(cols)

        if rows:
            table_tag = soup.new_tag("table")
            for row_index, row_cols in enumerate(rows):
                tr_tag = soup.new_tag("tr")
                for col_text in row_cols:
                    cell_tag = "th" if row_index == 0 else "td"
                    cell = soup.new_tag(cell_tag)
                    cell.string = col_text
                    tr_tag.append(cell)
                table_tag.append(tr_tag)
            se_table.replace_with(table_tag)
        else:
            se_table.decompose()

    # âœ… ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼)
    # se-module-codeë¥¼ ë¨¼ì € ì°¾ì•„ì„œ ì²˜ë¦¬ (ìƒìœ„ ì»¨í…Œì´ë„ˆ)
    code_modules = soup.find_all("div", class_="se-module-code")
    for code_module in code_modules:
        # ë‚´ë¶€ì˜ ì‹¤ì œ ì½”ë“œ ì†ŒìŠ¤ ì°¾ê¸°
        code_source = code_module.find("div", class_="se-code-source")
        if code_source:
            # __se_code_view ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            code_view = code_source.find("div", class_=lambda x: x and "__se_code_view" in " ".join(x) if isinstance(x, list) else x and "__se_code_view" in x)
            code_text = code_view.get_text() if code_view else code_source.get_text()
        else:
            code_text = code_module.get_text()

        # ë¹ˆ ì½”ë“œ ë¸”ë¡ì€ ì œê±°
        if not code_text.strip():
            code_module.decompose()
            continue

        code_blocks.append(code_text.strip())
        # ì½”ë“œ ë¸”ë¡ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´
        code_module.replace_with(f"\n\nÂ§Â§CODE{len(code_blocks)-1}Â§Â§\n\n")

    # âœ… ì¼ë°˜ pre, code íƒœê·¸ë„ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)
    for pre_tag in soup.find_all("pre"):
        code_text = pre_tag.get_text()
        if code_text.strip():
            code_blocks.append(code_text.strip())
            pre_tag.replace_with(f"\n\nÂ§Â§CODE{len(code_blocks)-1}Â§Â§\n\n")

    # âœ… í‘œ ì¶”ì¶œ (ê·¸ëŒ€ë¡œ ìœ ì§€)
    for idx, table in enumerate(soup.find_all("table")):
        rows = []
        for tr in table.find_all("tr"):
            cols = []
            for td in tr.find_all(["td", "th"]):
                text = td.get_text(" ", strip=True)
                text = text.replace("`", "")
                text = escape_md_table_cell(text)
                cols.append(text)
            if cols:
                rows.append(cols)
        if rows:
            md_table = format_markdown_table(rows)
            tables.append(md_table)
            table.replace_with(f"Â§Â§TABLE{idx}Â§Â§")
        else:
            table.decompose()

    # âœ… <hr> íƒœê·¸ â†’ html2textê°€ ê±´ë“œë¦¬ì§€ ëª»í•˜ë„ë¡ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ë³€ê²½
    for idx, hr in enumerate(soup.find_all("hr")):
        hr.replace_with(f"Â§Â§HR{idx}Â§Â§")

    # âœ… <br> â†’ ì¤„ë°”ê¿ˆ
    for br in soup.find_all("br"):
        br.replace_with("\n")

    text_preview = soup.get_text(separator=" ").strip()
    return str(soup), text_preview, tables, code_blocks


# âœ… HTML â†’ Markdown ë³€í™˜
def html_to_markdown(html: str, tables: list[str], code_blocks: list[str]) -> str:
    md_text = converter.handle(html)

    # âœ… hr ë³µì› (ë¬´ì¡°ê±´ ì¤„ë°”ê¿ˆ í¬í•¨)
    md_text = re.sub(r"\s*Â§Â§HR(\d+)Â§Â§\s*", r"\n\n---\n\n", md_text)

    # âœ… í‘œ ë³µì›
    for idx, table_md in enumerate(tables):
        md_text = md_text.replace(f"Â§Â§TABLE{idx}Â§Â§", f"\n\n{table_md}\n\n")

    # âœ… ì½”ë“œ ë¸”ë¡ ë³µì› (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í˜•ì‹ìœ¼ë¡œ)
    for idx, code_text in enumerate(code_blocks):
        # ì½”ë“œ ë¸”ë¡ì„ ```ë¡œ ê°ì‹¸ê¸°
        code_md = f"```\n{code_text.strip()}\n```"
        md_text = md_text.replace(f"Â§Â§CODE{idx}Â§Â§", code_md)

    # âœ… ì—¬ë¶„ì˜ ê°œí–‰ ì •ë¦¬ (4ì¤„ ì´ìƒë§Œ ì••ì¶•)
    md_text = re.sub(r"\n{4,}", "\n\n", md_text).strip()
    return md_text


# âœ… RSS ìˆœíšŒ
for entry in feedparser.parse(NAVER_RSS_URL).entries:
    title = (entry.title or "").strip()
    slug = (
        slug_pattern.sub("-", title.lower()).strip("-")
        or datetime.now().strftime("post-%Y%m%d-%H%M%S")
    )
    content_path = POSTS_DIR / f"{slug}.md"
    if content_path.exists():
        continue

    link = entry.get("link", "") or ""
    full_html, html_tags = fetch_full_naver_post(link)
    sanitized_html, text_preview, tables, code_blocks = sanitize_html(full_html or entry.get("description", ""))
    markdown = html_to_markdown(sanitized_html, tables, code_blocks)

    # âœ… ì²« ë¬¸ì¥ ìš”ì•½
    summary = entry.get("summary", "") or text_preview
    clean_summary = whitespace_re.sub(" ", summary).strip()
    sentence_end = re.search(r"[.!?]", clean_summary)
    if sentence_end:
        description = clean_summary[: sentence_end.end()].strip()
    else:
        description = clean_summary[:120]

    # âœ… íƒœê·¸ (RSS + ë³¸ë¬¸ í†µí•©)
    tags = html_tags or []
    raw_tags = entry.get("tags", []) or []
    for tag in raw_tags:
        if isinstance(tag, dict):
            value = tag.get("term") or tag.get("label")
            if value:
                tags.append(f"#{value.strip()}")
        elif tag:
            tags.append(f"#{str(tag).strip()}")

    for t in html_tags:
        if t not in tags:
            tags.append(t)

    # âœ… ë‚ ì§œ ì²˜ë¦¬
    parsed_date = entry.get("published_parsed") or entry.get("updated_parsed")
    if parsed_date:
        try:
            date = datetime(*parsed_date[:6]).strftime("%Y-%m-%d")
        except:
            date = datetime.now().strftime("%Y-%m-%d")
    else:
        date = datetime.now().strftime("%Y-%m-%d")

    reading_minutes = max(1, len(markdown.split()) // 200)

    # âœ… íŒŒì¼ ì‘ì„±
    sections = [f"# {title}", "", "---", "", markdown]
    if link:
        sections.append(f"\n[ì›ë¬¸ ë³´ê¸°]({link})")
    content_path.write_text("\n".join(sections).rstrip() + "\n", encoding="utf-8")

    posts_by_slug[slug] = {
        "slug": slug,
        "title": title,
        "description": description,
        "date": date,
        "tags": tags,
        "coverGradient": "from-emerald-500 via-teal-500 to-blue-500",
        "readingMinutes": reading_minutes,
        "contentPath": f"/posts/{slug}.md",
    }
    updated = True


# âœ… manifest & Git ë°˜ì˜
if updated:
    manifest = sorted(
        posts_by_slug.values(), key=lambda x: x["date"], reverse=True
    )
    MANIFEST_PATH.write_text(
        json.dumps(manifest, ensure_ascii=False, indent=2) + "\n", encoding="utf-8"
    )

    if GIT_USER_NAME and GIT_USER_EMAIL:
        subprocess.run(["git", "config", "user.name", GIT_USER_NAME], check=True)
        subprocess.run(["git", "config", "user.email", GIT_USER_EMAIL], check=True)

    current_branch = subprocess.check_output(
        ["git", "rev-parse", "--abbrev-ref", "HEAD"], text=True
    ).strip()
    if current_branch != "auto-post":
        subprocess.run(["git", "checkout", "-B", "auto-post"], check=True)

    subprocess.run(["git", "add", str(POSTS_DIR)], check=True)
    commit_msg = f"Auto post update {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    subprocess.run(["git", "commit", "-m", commit_msg], check=True)

    # ì›ê²© ë¸Œëœì¹˜ ì¡´ì¬ í™•ì¸ ë° ì‚­ì œ
    result = subprocess.run(
        ["git", "ls-remote", "--heads", "origin", "auto-post"],
        capture_output=True, text=True
    )
    if result.stdout.strip():
        subprocess.run(["git", "push", "origin", "--delete", "auto-post"], check=False)

    subprocess.run(["git", "push", "-u", "origin", "auto-post"], check=True)
    print("ğŸš€ Push ì™„ë£Œ")
else:
    print("â„¹ï¸ ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ì—†ìŒ")
